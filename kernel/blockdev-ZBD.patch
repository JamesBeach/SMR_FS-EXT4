diff --git a/block/Kconfig b/block/Kconfig
index 161491d..e3c432d 100644
--- a/block/Kconfig
+++ b/block/Kconfig
@@ -100,6 +100,24 @@ config BLK_DEV_THROTTLING
 
 	See Documentation/cgroups/blkio-controller.txt for more information.
 
+config BLK_DEV_ZONED
+	bool "Zoned block device support"
+	default n
+	---help---
+	Block layer zoned block device support. This option enables
+	support for zoned block (ZAC/ZBC) devices.
+
+	Say yes here if you have a ZAC or ZBC storage device.
+
+config BLK_ZONED
+	tristate "Support for ZONED block devices."
+	default y
+	---help---
+	Block layer Zoned device support. Zoned devices are SMR Host Aware
+	and SMR Host Managed devices. Typically 8TB and higher density
+	drives are Zoned SMR devices. This supports identifying Zone
+	information on such drives.
+
 config BLK_CMDLINE_PARSER
 	bool "Block device command line partition parser"
 	default n
diff --git a/block/Makefile b/block/Makefile
index 00ecc97..dccda07 100644
--- a/block/Makefile
+++ b/block/Makefile
@@ -14,6 +14,8 @@ obj-$(CONFIG_BOUNCE)	+= bounce.o
 obj-$(CONFIG_BLK_DEV_BSG)	+= bsg.o
 obj-$(CONFIG_BLK_DEV_BSGLIB)	+= bsg-lib.o
 obj-$(CONFIG_BLK_CGROUP)	+= blk-cgroup.o
+obj-$(CONFIG_BLK_DEV_ZONED)	+= blk-zoned.o
+obj-$(CONFIG_BLK_ZONED)		+= blk-zoned-ctrl.o
 obj-$(CONFIG_BLK_DEV_THROTTLING)	+= blk-throttle.o
 obj-$(CONFIG_IOSCHED_NOOP)	+= noop-iosched.o
 obj-$(CONFIG_IOSCHED_DEADLINE)	+= deadline-iosched.o
diff --git a/block/blk-core.c b/block/blk-core.c
index 794c3e7..db92631 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -547,6 +547,8 @@ void blk_cleanup_queue(struct request_queue *q)
 	if (q->mq_ops)
 		blk_mq_free_queue(q);
 
+	blk_drop_zones(q);
+
 	spin_lock_irq(lock);
 	if (q->queue_lock != &q->__queue_lock)
 		q->queue_lock = &q->__queue_lock;
@@ -624,6 +626,10 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 #ifdef CONFIG_BLK_CGROUP
 	INIT_LIST_HEAD(&q->blkg_list);
 #endif
+#ifdef CONFIG_BLK_DEV_ZONED
+	q->zones = RB_ROOT;
+#endif
+
 	INIT_DELAYED_WORK(&q->delay_work, blk_delay_work);
 
 	kobject_init(&q->kobj, &blk_queue_ktype);
@@ -1293,10 +1299,37 @@ void blk_requeue_request(struct request_queue *q, struct request *rq)
 
 	BUG_ON(blk_queued_rq(rq));
 
-	elv_requeue_request(q, rq);
+	elv_requeue_request(q, rq, false);
 }
 EXPORT_SYMBOL(blk_requeue_request);
 
+/**
+ * blk_defer_request - put a request back on the tail of the queue
+ * @q:		request queue where request should be inserted
+ * @rq:		request to be inserted
+ *
+ * Description:
+ *    Some drivers require strict request ordering, which cannot
+ *    be guaranteed by the block layer. For these drivers we
+ *    need to put the request back to the end of the queue so that
+ *    the correct request will be served first.
+ *    Must be called with queue lock held.
+ */
+void blk_defer_request(struct request_queue *q, struct request *rq)
+{
+	blk_delete_timer(rq);
+	blk_clear_rq_complete(rq);
+	trace_block_rq_requeue(q, rq);
+
+	if (rq->cmd_flags & REQ_QUEUED)
+		blk_queue_end_tag(q, rq);
+
+	BUG_ON(blk_queued_rq(rq));
+
+	elv_requeue_request(q, rq, true);
+}
+EXPORT_SYMBOL(blk_defer_request);
+
 static void add_acct_request(struct request_queue *q, struct request *rq,
 			     int where)
 {
@@ -2316,6 +2349,17 @@ struct request *blk_peek_request(struct request_queue *q)
 			 */
 			blk_start_request(rq);
 			__blk_end_request_all(rq, -EIO);
+		} else if (ret == BLKPREP_DONE) {
+			rq->cmd_flags |= REQ_QUIET;
+			/*
+			 * Mark this request as started so we don't trigger
+			 * any debug logic in the end I/O path.
+			 */
+			blk_start_request(rq);
+			__blk_end_request_all(rq, 0);
+		} else if (ret == BLKPREP_REQUEUE) {
+			blk_start_request(rq);
+			blk_defer_request(q, rq);
 		} else {
 			printk(KERN_ERR "%s: bad return=%d\n", __func__, ret);
 			break;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 6ed2cbe..875979d 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -296,6 +296,26 @@ void blk_queue_chunk_sectors(struct request_queue *q, unsigned int chunk_sectors
 EXPORT_SYMBOL(blk_queue_chunk_sectors);
 
 /**
+ * blk_queue_chunk_limits - define a chunk_limts functions for a device
+ * @q:  the request queue for the device
+ * @chunk_limits_fn:  Function to limit the chunk size
+ *
+ * Description:
+ *    If a driver doesn't want IOs to cross a given chunk size, it can
+ *    define this function and prevent merging across chunks.
+ *    This function is for primarily for chunks of different sizes,
+ *    so that the request itself must be examined. Also note that the block
+ *    layer must accept a page worth of data at any offset. So if the
+ *    crossing of chunks is a hard limitation in the driver, it must still be
+ *    prepared to split single page bios.
+ **/
+void blk_queue_chunk_limits(struct request_queue *q, chunk_limits_fn *cfn)
+{
+	q->chunk_limits_fn = cfn;
+}
+EXPORT_SYMBOL(blk_queue_chunk_limits);
+
+/**
  * blk_queue_max_discard_sectors - set max sectors for a single discard
  * @q:  the request queue for the device
  * @max_discard_sectors: maximum number of sectors to discard
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index faaf36a..74426d9 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -190,6 +190,43 @@ static ssize_t queue_max_hw_sectors_show(struct request_queue *q, char *page)
 	return queue_var_show(max_hw_sectors_kb, (page));
 }
 
+#ifdef CONFIG_BLK_DEV_ZONED
+static ssize_t queue_zoned_show(struct request_queue *q, char *page)
+{
+	struct rb_node *node;
+	struct blk_zone *zone;
+	ssize_t offset = 0, end = 0;
+	int size = 0, num = 0;
+	enum blk_zone_type type = BLK_ZONE_TYPE_UNKNOWN;
+
+	for (node = rb_first(&q->zones); node; node = rb_next(node)) {
+		zone = rb_entry(node, struct blk_zone, node);
+		if (zone->type != type ||
+		    zone->len != size ||
+		    end != zone->start) {
+			if (size != 0)
+				offset += sprintf(page + offset, "%u\n", num);
+			/* We can only store one page ... */
+			if (offset + 42 > PAGE_SIZE) {
+				offset += sprintf(page + offset, "...\n");
+				return offset;
+			}
+			size = zone->len;
+			type = zone->type;
+			offset += sprintf(page + offset, "%llu %u %d ",
+					  zone->start, size, type);
+			num = 0;
+			end = zone->start + size;
+		} else
+			end += zone->len;
+		num++;
+	}
+	if (num > 0)
+		offset += sprintf(page + offset, "%u\n", num);
+	return offset > 0 ? offset : -EINVAL;
+}
+#endif
+
 #define QUEUE_SYSFS_BIT_FNS(name, flag, neg)				\
 static ssize_t								\
 queue_show_##name(struct request_queue *q, char *page)			\
@@ -374,6 +411,13 @@ static struct queue_sysfs_entry queue_write_same_max_entry = {
 	.show = queue_write_same_max_show,
 };
 
+#ifdef CONFIG_BLK_DEV_ZONED
+static struct queue_sysfs_entry queue_zoned_entry = {
+	.attr = {.name = "zoned", .mode = S_IRUGO },
+	.show = queue_zoned_show,
+};
+#endif
+
 static struct queue_sysfs_entry queue_nonrot_entry = {
 	.attr = {.name = "rotational", .mode = S_IRUGO | S_IWUSR },
 	.show = queue_show_nonrot,
@@ -423,6 +467,9 @@ static struct attribute *default_attrs[] = {
 	&queue_discard_zeroes_data_entry.attr,
 	&queue_write_same_max_entry.attr,
 	&queue_nonrot_entry.attr,
+#ifdef CONFIG_BLK_DEV_ZONED
+	&queue_zoned_entry.attr,
+#endif
 	&queue_nomerges_entry.attr,
 	&queue_rq_affinity_entry.attr,
 	&queue_iostats_entry.attr,
diff --git a/block/blk-zoned-ctrl.c b/block/blk-zoned-ctrl.c
new file mode 100755
index 0000000..405e317
--- /dev/null
+++ b/block/blk-zoned-ctrl.c
@@ -0,0 +1,416 @@
+/*
+ * Functions for zone based SMR devices.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ * XiaoDong Han <xiaodong.h.han@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/bio.h>
+#include <linux/blkdev.h>
+#include <linux/blk-mq.h>
+#include <linux/sched/sysctl.h>
+
+#include "blk.h"
+#include <linux/blk-zoned-ctrl.h>
+
+/*
+ * for max sense size
+ */
+#include <scsi/scsi.h>
+#include <scsi/scsi_cmnd.h>
+#include <scsi/scsi_eh.h>
+#include <scsi/scsi_dbg.h>
+#include <linux/ata.h>
+
+#define ZBC_TIMEOUT		   (30 * HZ)
+#define ZBC_MAX_RETRIES		     5
+#define CMD_LEN                     16
+#define INQUIRY_CMDLEN  6
+
+/** ----------------------------------------------------------------- */
+/** ----------------------------------------------------------------- */
+/** ------------------- SMR ZONED DRIVE SUPPORT --------------------- */
+/** ----------------------------------------------------------------- */
+/** ----------------------------------------------------------------- */
+
+static inline void _len_to_cmd_zbc(u8 * cmd, u32 _len)
+{
+	u32 len = cpu_to_be32(_len);
+	memcpy(cmd, &len, sizeof(len));
+}
+
+static inline void _lba_to_cmd_zbc(u8 * cmd, u64 _lba)
+{
+	u64 lba = cpu_to_be64(_lba);
+	memcpy(cmd, &lba, sizeof(lba));
+}
+
+static inline u16 zc_get_word(u8 * buf)
+{
+	u16 w = buf[1];
+	w <<= 8;
+	w |= buf[0];
+	return w;
+}
+
+/* NOTE: this is basically scsi_execute */
+int blk_cmd_execute(struct request_queue *queue,
+			   const unsigned char *cmd,
+			   int data_direction,
+			   void *buffer,
+			   unsigned bufflen,
+			   unsigned char *sense,
+			   int timeout,
+			   int retries,
+			   u64 flags,
+			   int *resid)
+{
+        struct request *req;
+        int write = (data_direction == DMA_TO_DEVICE);
+        int ret = DRIVER_ERROR << 24;
+
+        req = blk_get_request(queue, write, __GFP_WAIT);
+        if (IS_ERR(req))
+                return ret;
+        blk_rq_set_block_pc(req);
+
+        if (bufflen &&  blk_rq_map_kern(queue, req,
+                                        buffer, bufflen, __GFP_WAIT))
+                goto out;
+
+        req->cmd_len = COMMAND_SIZE(cmd[0]);
+        memcpy(req->cmd, cmd, req->cmd_len);
+        req->sense = sense;
+        req->sense_len = 0;
+        req->retries = retries;
+        req->timeout = timeout;
+        req->cmd_flags |= flags | REQ_QUIET | REQ_PREEMPT;
+
+        /*
+         * head injection *required* here otherwise quiesce won't work
+         */
+        blk_execute_rq(req->q, NULL, req, 1);
+
+        /*
+         * Some devices (USB mass-storage in particular) may transfer
+         * garbage data together with a residue indicating that the data
+         * is invalid.  Prevent the garbage from being misinterpreted
+         * and prevent security leaks by zeroing out the excess data.
+         */
+        if (unlikely(req->resid_len > 0 && req->resid_len <= bufflen))
+                memset(buffer + (bufflen - req->resid_len), 0, req->resid_len);
+
+        if (resid)
+                *resid = req->resid_len;
+        ret = req->errors;
+ out:
+        blk_put_request(req);
+
+        return ret;
+}
+EXPORT_SYMBOL(blk_cmd_execute);
+
+int blk_cmd_with_sense(struct gendisk *disk,
+	u8 * cmd, int data_direction,
+	u8 * buf, u32 buf_len, u8 * sense_buffer)
+{
+	struct request_queue *queue = disk->queue;
+	int rc;
+	struct scsi_sense_hdr sshdr = { 0 };
+
+	if (!sense_buffer) {
+		pr_err("scsi cmd exec: sense buffer is NULL\n");
+		return -1;
+	}
+
+	rc = blk_cmd_execute(queue, cmd, data_direction, buf, buf_len,
+		sense_buffer, ZBC_TIMEOUT, ZBC_MAX_RETRIES, 0, NULL);
+
+	pr_debug("%s: %s -> 0x%08x"
+		" [h:%02x d:%02x m:%02x s:%02x]\n", __func__,
+		disk->disk_name, rc,
+			host_byte(rc),
+			driver_byte(rc),
+			msg_byte(rc),
+			status_byte(rc));
+
+	scsi_normalize_sense(sense_buffer, SCSI_SENSE_BUFFERSIZE, &sshdr);
+	if (host_byte(rc)
+	    || (    driver_byte(rc)
+		&& (driver_byte(rc) != DRIVER_SENSE) )
+	    || (    status_byte(rc)
+		&& (status_byte(rc) != CHECK_CONDITION)) ) {
+		pr_err("exec scsi cmd failed,opcode:%d\n", cmd[0]);
+		if (driver_byte(rc) & DRIVER_SENSE) {
+			pr_err("%s: %s", __func__, disk->disk_name );
+		}
+		return -1;
+	} else if (   (driver_byte(rc) == DRIVER_SENSE)
+		   && ((cmd[0] == ATA_16) || (cmd[0] == ATA_12))) {
+		if (sense_buffer[21] != 0x50) {
+			pr_err("%s: ATA pass through command failed\n",
+				__func__);
+			return -1;
+		}
+	} else if (rc) {
+		if (   (driver_byte(rc) == DRIVER_SENSE)
+		    && (status_byte(rc) == CHECK_CONDITION)
+		    && (0 != sense_buffer[0])) {
+			pr_err("%s: Something else failed\n", __func__);
+			return -1;
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(blk_cmd_with_sense);
+
+int blk_zoned_report(struct gendisk *disk,
+			u64 start_lba,
+			u8 opt,
+			u8 * buf,
+			size_t bufsz)
+{
+	int ret = 0;
+	u8 cmd[CMD_LEN] = {0};
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = {0};
+
+	cmd[0] = ZBC_IN;
+	cmd[1] = ZI_REPORT_ZONES;
+
+	_lba_to_cmd_zbc(&cmd[2],  start_lba);
+	_len_to_cmd_zbc(&cmd[10], (u32)bufsz);
+
+	cmd[14] = opt;
+
+	pr_debug("%s: "
+		"%02x:%02x "
+		"lba:%02x%02x%02x%02x%02x%02x%02x%02x "
+		"len:%02x%02x%02x%02x %02x %02x\n",
+		__func__,
+		cmd[0],  cmd[1],
+		cmd[2],  cmd[3],  cmd[4],  cmd[5],
+		cmd[6],  cmd[7],  cmd[8],  cmd[9],
+		cmd[10], cmd[11], cmd[12], cmd[13],
+		cmd[14], cmd[15] );
+
+	ret = blk_cmd_with_sense(disk, cmd, DMA_FROM_DEVICE, buf, bufsz,
+					&sense_buf[0]);
+	return ret;
+}
+EXPORT_SYMBOL(blk_zoned_report);
+
+int blk_zoned_inquiry(struct gendisk *disk, u8 evpd, u8 pg_op,
+	u16 mx_resp_len, u8 * buf)
+{
+	int ret = 0;
+	u8 cmd[INQUIRY_CMDLEN] = {0};
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = {0};
+
+	__be16 slen = cpu_to_be16(mx_resp_len);
+
+	if (0xb1 != pg_op) {
+		pr_err("Page Code %02x is wrong, the correct page"
+		       " code is 0xb1\n", pg_op);
+		return -1;
+	}
+
+	cmd[0] = INQUIRY;
+	if (evpd)
+		cmd[1] |= 1;
+	cmd[2] = pg_op;
+	cmd[3] = slen & 0xff;
+	cmd[4] = (slen >> 8) & 0xff;
+
+	pr_debug("%s: cmd: "
+		"%02x:%02x:%02x:%02x:%02x:%02x\n",
+		__func__,
+		cmd[0],  cmd[1], cmd[2],  cmd[3],  cmd[4],  cmd[5]);
+
+	ret = blk_cmd_with_sense(disk, cmd, DMA_FROM_DEVICE,
+					buf, mx_resp_len, &sense_buf[0]);
+	if (ret != 0) {
+		pr_err("%s: inquiry failed\n", disk->disk_name);
+		goto out;
+	}
+
+out:
+	return ret;
+}
+EXPORT_SYMBOL(blk_zoned_inquiry);
+
+int blk_zoned_reset_wp(struct gendisk *disk, u64 start_lba)
+{
+	int ret = 0;
+	u8 cmd[CMD_LEN] = {0};
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = {0};
+	u8 all_bit = 0;
+
+	pr_debug("reset wp: %s, start_lba %lld\n", disk->disk_name,
+		start_lba);
+
+	if (start_lba == ~0ul) {
+		all_bit = 1;
+		start_lba = 0;
+	}
+
+	cmd[0] = ZBC_OUT;
+	cmd[1] = ZO_RESET_WRITE_POINTER;
+
+	_lba_to_cmd_zbc(&cmd[2], start_lba);
+
+	pr_debug("%s: "
+		"%02x:%02x "
+		"lba:%02x%02x%02x%02x%02x%02x%02x%02x "
+		"len:%02x%02x%02x%02x %02x %02x\n",
+		__func__,
+		cmd[0],  cmd[1],
+		cmd[2],  cmd[3],  cmd[4],  cmd[5],
+		cmd[6],  cmd[7],  cmd[8],  cmd[9],
+		cmd[10], cmd[11], cmd[12], cmd[13],
+		cmd[14], cmd[15] );
+
+	cmd[14] = all_bit;
+	ret = blk_cmd_with_sense(disk, cmd, DMA_FROM_DEVICE, NULL, 0,
+		&sense_buf[0]);
+	if (ret != 0) {
+		pr_err("%s: reset write pointer failed\n",
+			disk->disk_name);
+		return -1;
+	}
+	if (all_bit) {
+		pr_debug("reset all zone's write pointer success\n");
+	} else {
+		pr_debug("reset zone %llu write pointer success\n",
+			start_lba);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(blk_zoned_reset_wp);
+
+int _inquiry_ioctl(struct gendisk *disk, void __user *parg)
+{
+	int error = 0;
+	size_t result_size = 0;
+	size_t alloc_size = PAGE_SIZE;
+	zoned_inquiry_t * inq = kmalloc(alloc_size, GFP_KERNEL);
+
+	if (!inq) {
+		error = -ENOMEM;
+		goto out;
+	}
+	if (copy_from_user(inq, parg, sizeof(*inq))) {
+		error = -EFAULT;
+		goto out;
+	}
+	result_size = inq->mx_resp_len + offsetof(zoned_inquiry_t, result);
+	if (result_size > alloc_size ) {
+		void * tmp;
+		alloc_size = result_size;
+		tmp = krealloc(inq, alloc_size, GFP_KERNEL);
+		if (!tmp) {
+			error = -ENOMEM;
+			goto out;
+		}
+		inq = tmp;
+	}
+	error = blk_zoned_inquiry(disk, inq->evpd,  inq->pg_op,
+				  inq->mx_resp_len, inq->result);
+	if (error) {
+		error = -EFAULT;
+		goto out;
+	}
+	if (copy_to_user(parg, inq, result_size)) {
+		error = -EFAULT;
+		goto out;
+	}
+
+out:
+	if (inq) {
+		kfree(inq);
+	}
+	return error;
+}
+EXPORT_SYMBOL(_inquiry_ioctl);
+
+int _reset_wp_ioctl(struct gendisk *disk, unsigned long arg)
+{
+	int error = -EFAULT;
+	error = blk_zoned_reset_wp(disk, arg);
+	return error;
+}
+EXPORT_SYMBOL(_reset_wp_ioctl);
+
+int _report_zones_ioctl(struct gendisk *disk, void __user *parg)
+{
+	int error = -EFAULT;
+	int is_vm = 0;
+	struct bdev_zone_report_ioctl_t * zone_iodata = NULL;
+	u32 alloc_size = max(PAGE_SIZE, sizeof(*zone_iodata));
+
+	zone_iodata = kmalloc(alloc_size, GFP_KERNEL);
+	if (!zone_iodata) {
+		error = -ENOMEM;
+		goto report_zones_out;
+	}
+	if (copy_from_user(zone_iodata, parg, sizeof(*zone_iodata))) {
+		error = -EFAULT;
+		goto report_zones_out;
+	}
+	if (zone_iodata->data.in.return_page_count > alloc_size) {
+		void * tmp;
+		alloc_size = zone_iodata->data.in.return_page_count;
+		if (alloc_size < KMALLOC_MAX_SIZE) {
+			tmp = krealloc(zone_iodata, alloc_size, GFP_KERNEL);
+			if (!tmp) {
+				error = -ENOMEM;
+				goto report_zones_out;
+			}
+			zone_iodata = tmp;
+		} else {
+			/* too large for kmalloc, fallback to vmalloc */
+			is_vm = 1;
+			tmp = zone_iodata;
+			zone_iodata = vzalloc(alloc_size);
+			if (zone_iodata) {
+				memcpy(zone_iodata, tmp,
+					sizeof(*zone_iodata));
+			}
+			kfree(tmp);
+			if (!zone_iodata) {
+				error = -ENOMEM;
+				goto report_zones_out;
+			}
+		}
+	}
+	error = blk_zoned_report(disk,
+			zone_iodata->data.in.zone_locator_lba,
+			zone_iodata->data.in.report_option,
+			(u8*)&zone_iodata->data.out,
+			alloc_size );
+	if (error) {
+		goto report_zones_out;
+	}
+	if (copy_to_user(parg, zone_iodata, alloc_size)) {
+		error = -EFAULT;
+	}
+report_zones_out:
+	if (zone_iodata) {
+		if (is_vm)
+			vfree(zone_iodata);
+		else
+			kfree(zone_iodata);
+	}
+	return error;
+}
+EXPORT_SYMBOL(_report_zones_ioctl);
diff --git a/block/blk-zoned.c b/block/blk-zoned.c
new file mode 100644
index 0000000..354bc0f
--- /dev/null
+++ b/block/blk-zoned.c
@@ -0,0 +1,95 @@
+/*
+ * Zoned block device handling
+ *
+ * Copyright (c) 2015, Hannes Reinecke
+ * Copyright (c) 2015, SUSE Linux GmbH
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/blkdev.h>
+#include <linux/rbtree.h>
+
+struct blk_zone *blk_lookup_zone(struct request_queue *q, sector_t lba)
+{
+	struct rb_root *root = &q->zones;
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct blk_zone *zone = container_of(node, struct blk_zone,
+						     node);
+
+		if (lba < zone->start)
+			node = node->rb_left;
+		else if (lba >= zone->start + zone->len)
+			node = node->rb_right;
+		else
+			return zone;
+	}
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(blk_lookup_zone);
+
+struct blk_zone *blk_insert_zone(struct request_queue *q, struct blk_zone *data)
+{
+	struct rb_root *root = &q->zones;
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct blk_zone *this = container_of(*new, struct blk_zone,
+						     node);
+		parent = *new;
+		if (data->start + data->len <= this->start)
+			new = &((*new)->rb_left);
+		else if (data->start >= this->start + this->len)
+			new = &((*new)->rb_right);
+		else {
+			/* Return existing zone */
+			return this;
+		}
+	}
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(blk_insert_zone);
+
+void blk_drop_zones(struct request_queue *q)
+{
+	struct rb_root *root = &q->zones;
+	struct blk_zone *zone, *next;
+
+	rbtree_postorder_for_each_entry_safe(zone, next, root, node) {
+		kfree(zone);
+	}
+	q->zones = RB_ROOT;
+	q->zone_len = 0;
+}
+EXPORT_SYMBOL_GPL(blk_drop_zones);
+
+sector_t blk_zone_get_wp(struct blk_zone *zone)
+{
+	unsigned long flags;
+	sector_t wp;
+
+	spin_lock_irqsave(&zone->lock, flags);
+	wp = zone->wp;
+	spin_unlock_irqrestore(&zone->lock, flags);
+	return wp;
+}
+EXPORT_SYMBOL_GPL(blk_zone_get_wp);
+
+void blk_zone_copy(struct blk_zone *from, struct blk_zone *to)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&from->lock, flags);
+	to->start = from->start;
+	to->len = from->len;
+	to->wp = from->wp;
+	spin_unlock_irqrestore(&from->lock, flags);
+}
+EXPORT_SYMBOL_GPL(blk_zone_copy);

diff --git a/block/elevator.c b/block/elevator.c
index 59794d0..e7cf42b 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -560,7 +560,8 @@ static inline void blk_pm_add_request(struct request_queue *q,
 }
 #endif
 
-void elv_requeue_request(struct request_queue *q, struct request *rq)
+void elv_requeue_request(struct request_queue *q, struct request *rq,
+			 bool defer)
 {
 	/*
 	 * it already went through dequeue, we need to decrement the
@@ -576,7 +577,8 @@ void elv_requeue_request(struct request_queue *q, struct request *rq)
 
 	blk_pm_requeue_request(rq);
 
-	__elv_add_request(q, rq, ELEVATOR_INSERT_REQUEUE);
+	__elv_add_request(q, rq, defer ?
+			  ELEVATOR_INSERT_DEFER : ELEVATOR_INSERT_REQUEUE);
 }
 
 void elv_drain_elevator(struct request_queue *q)
@@ -620,6 +622,7 @@ void __elv_add_request(struct request_queue *q, struct request *rq, int where)
 		list_add(&rq->queuelist, &q->queue_head);
 		break;
 
+	case ELEVATOR_INSERT_DEFER:
 	case ELEVATOR_INSERT_BACK:
 		rq->cmd_flags |= REQ_SOFTBARRIER;
 		elv_drain_elevator(q);

diff --git a/include/linux/blk-zoned-ctrl.h b/include/linux/blk-zoned-ctrl.h
new file mode 100755
index 0000000..c26fb67
--- /dev/null
+++ b/include/linux/blk-zoned-ctrl.h
@@ -0,0 +1,200 @@
+/*
+ * Functions for zone based SMR devices.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef BLK_ZONED_H
+#define BLK_ZONED_H
+
+enum zone_report_option {
+	ZOPT_NON_SEQ_AND_RESET   = 0x00,
+	ZOPT_ZC1_EMPTY,
+	ZOPT_ZC2_OPEN_IMPLICIT,
+	ZOPT_ZC3_OPEN_EXPLICIT,
+	ZOPT_ZC4_CLOSED,
+	ZOPT_ZC5_FULL,
+	ZOPT_ZC6_READ_ONLY,
+	ZOPT_ZC7_OFFLINE,
+	ZOPT_RESET               = 0x10,
+	ZOPT_NON_SEQ             = 0x11,
+	ZOPT_NON_WP_ZONES        = 0x3f,
+};
+
+/* Report, close, finish, open, reset wp: */
+enum zone_zm_action {
+	REPORT_ZONES_EXT   = 0x00,
+	CLOSE_ZONE_EXT,
+	FINISH_ZONE_EXT,
+	OPEN_ZONE_EXT,
+	RESET_WP_EXT,
+};
+
+struct bdev_zone_report_request_t {
+	__u64 zone_locator_lba;	  /* starting lba for first zone to be reported. */
+	__u32 return_page_count;  /* number of bytes allocated for result */
+	__u8  report_option;	  /* see: zone_report_option enum */
+};
+
+enum bdev_zone_type {
+	ZTYP_RESERVED            = 0,
+	ZTYP_CONVENTIONAL        = 1,
+	ZTYP_SEQ_WRITE_REQUIRED  = 2,
+	ZTYP_SEQ_WRITE_PREFERRED = 3,
+};
+
+enum bdev_zone_condition {
+	ZCOND_CONVENTIONAL       = 0, /* no write pointer */
+	ZCOND_ZC1_EMPTY          = 1,
+	ZCOND_ZC2_OPEN_IMPLICIT  = 2,
+	ZCOND_ZC3_OPEN_EXPLICIT  = 3,
+	ZCOND_ZC4_CLOSED         = 4,
+	/* 5 - 0xC - reserved */
+	ZCOND_ZC6_READ_ONLY      = 0xd,
+	ZCOND_ZC5_FULL           = 0xe,
+	ZCOND_ZC7_OFFLINE        = 0xf,
+};
+
+/* NOTE: all LBA's are u64 only use the lower 48 bits */
+
+struct bdev_zone_descriptor_entry_t {
+	u8  type;         /* see zone_type enum */
+	u8  flags;        /* 0:reset, 1:non-seq, 2-3: resv,
+                           * bits 4-7: see zone_condition enum */
+	u8  reserved1[6];
+	u64 length;       /* length of zone: in sectors */
+	u64 lba_start;    /* lba of zone start */
+	u64 lba_wptr;     /* lba of write pointer - ready to be written
+			   * next */
+        u8 reserved[32];
+} __packed;
+
+enum bdev_zone_same {
+	ZS_ALL_DIFFERENT        = 0,
+	ZS_ALL_SAME             = 1,
+	ZS_LAST_DIFFERS         = 2,
+	ZS_SAME_LEN_DIFF_TYPES  = 3,
+};
+
+struct bdev_zone_report_result_t {
+	u32 descriptor_count;   /* number of zone_descriptor entries that
+				 * follow */
+	u8  same_field;         /* bits 0-3: enum zone_same (MASK: 0x0F) */
+	u8  reserved1[3];
+	u64 maximum_lba;        /* The MAXIMUM LBA field indicates the
+				 * LBA of the last logical sector on the
+				 * device, including all logical sectors
+				 * in all zones. */
+	u8  reserved2[48];
+	struct bdev_zone_descriptor_entry_t descriptors[0];
+} __packed;
+
+struct bdev_zone_report_ioctl_t {
+	union {
+		struct bdev_zone_report_request_t in;
+		struct bdev_zone_report_result_t out;
+	} data;
+} __packed;
+
+
+/**
+ * According to the test result,
+ *  SEAGATE driver do not support this option,
+ *  need renew this definition in future
+ */
+typedef enum zc_report_options {
+	ZC_RO_RESET = 0x00,
+	ZC_RO_OFFLINE = 0x01,
+	ZC_RO_RDONLY = 0x02,
+	ZC_RO_FULL = 0x03,
+	ZC_RO_OP_NOT_READY = 0x4,
+	ZC_RO_ALL = 0xF,
+} zc_report_options_t;
+
+/**
+ * Flags to determine if the connected disk is ZONED:
+ *   - Host Aware of Host Managed (or not)
+ */
+typedef enum zc_type {
+	NOT_ZONED    = 0x00,
+	HOST_AWARE   = 0x01,
+	HOST_MANAGE  = 0x02,
+} zc_type_t;
+
+typedef enum zc_vendor_type {
+	ZONE_DEV_ATA_SEAGATE = 0x00,
+	ZONE_DEV_BLK         = 0x01,
+} zc_vendor_type_t;
+
+struct zoned_inquiry {
+	u8  evpd;
+	u8  pg_op;
+	u16 mx_resp_len;
+	u8  result[0];
+} __packed;
+typedef struct zoned_inquiry zoned_inquiry_t;
+
+/*0xC6: Seagate SMR aware band configuration log*/
+typedef enum zc_zone_type {
+	ZC_TYPE_CONVENTIONAL  = 0x00,
+	ZC_TYPE_HOST_ASSISTED = 0x01,	/* sequential write preferred */
+	ZC_TYPE_HOST_MANAGED  = 0x02,	/* sequential write required  */
+} zc_zone_type_t;
+
+/*0xC6: Seagate SMR aware band configuration log*/
+typedef enum zc_zone_condition {
+	ZC_CON_RESET        = 0x00,
+	ZC_CON_OFFLINE      = 0x01,
+	ZC_CON_RDONLY       = 0x02,
+	ZC_CON_FULL         = 0x03,
+	ZC_CON_OP_NOT_READY = 0x4,
+} zc_zone_condition_t;
+
+/* zone descriptor format, device controller reports this infomation
+ * REPORT ZONES */
+typedef struct zc_zone {
+	zc_zone_type_t      zone_type;
+	zc_zone_condition_t zone_condition;
+	u64 zone_id;		/* Optional */
+	u64 zone_length;
+	u64 zone_start_lba;
+	u64 zone_write_pointer;
+} zc_zone_t;
+
+
+/* this is basically scsi_execute */
+int blk_cmd_execute(struct request_queue *queue,
+			   const unsigned char *cmd,
+			   int data_direction,
+			   void *buffer,
+			   unsigned bufflen,
+			   unsigned char *sense,
+			   int timeout,
+			   int retries,
+			   u64 flags,
+			   int *resid);
+int blk_cmd_with_sense(struct gendisk *disk,
+	u8 * cmd, int data_direction,
+	u8 * buf, u32 buf_len, u8 * sense_buffer);
+int blk_zoned_report(struct gendisk *disk,
+			u64 start_lba,
+			u8 opt,
+			u8 * buf,
+			size_t bufsz);
+int blk_zoned_inquiry(struct gendisk *disk, u8 evpd, u8 pg_op,
+	u16 mx_resp_len, u8 * buf);
+int blk_zoned_reset_wp(struct gendisk *disk, u64 start_lba);
+
+int _inquiry_ioctl(struct gendisk *disk, void __user *parg);
+int _reset_wp_ioctl(struct gendisk *disk, unsigned long arg);
+int _report_zones_ioctl(struct gendisk *disk, void __user *parg);
+
+
+#endif /* INT_BLK_ZONED_H */
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index c294e3e..2d1ca00 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -165,6 +165,7 @@ enum rq_flag_bits {
 	__REQ_INTEGRITY,	/* I/O includes block integrity payload */
 	__REQ_FUA,		/* forced unit access */
 	__REQ_FLUSH,		/* request for cache flush */
+	__REQ_WPUPDATE,		/* shadow write pointer updated */
 
 	/* bio only flags */
 	__REQ_RAHEAD,		/* read ahead, can fail anytime */
@@ -205,13 +206,14 @@ enum rq_flag_bits {
 #define REQ_WRITE_SAME		(1ULL << __REQ_WRITE_SAME)
 #define REQ_NOIDLE		(1ULL << __REQ_NOIDLE)
 #define REQ_INTEGRITY		(1ULL << __REQ_INTEGRITY)
+#define REQ_WPUPDATE		(1ULL << __REQ_WPUPDATE)
 
 #define REQ_FAILFAST_MASK \
 	(REQ_FAILFAST_DEV | REQ_FAILFAST_TRANSPORT | REQ_FAILFAST_DRIVER)
 #define REQ_COMMON_MASK \
 	(REQ_WRITE | REQ_FAILFAST_MASK | REQ_SYNC | REQ_META | REQ_PRIO | \
 	 REQ_DISCARD | REQ_WRITE_SAME | REQ_NOIDLE | REQ_FLUSH | REQ_FUA | \
-	 REQ_SECURE | REQ_INTEGRITY)
+	 REQ_SECURE | REQ_INTEGRITY | REQ_WPUPDATE)
 #define REQ_CLONE_MASK		REQ_COMMON_MASK
 
 #define BIO_NO_ADVANCE_ITER_MASK	(REQ_DISCARD|REQ_WRITE_SAME)
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7f9a516..11f919b 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -251,6 +251,7 @@ typedef void (softirq_done_fn)(struct request *);
 typedef int (dma_drain_needed_fn)(struct request *);
 typedef int (lld_busy_fn) (struct request_queue *q);
 typedef int (bsg_job_fn) (struct bsg_job *);
+typedef int (chunk_limits_fn) (struct request_queue *q, sector_t offset);
 
 enum blk_eh_timer_return {
 	BLK_EH_NOT_HANDLED,
@@ -281,6 +282,52 @@ struct blk_queue_tag {
 #define BLK_SCSI_MAX_CMDS	(256)
 #define BLK_SCSI_CMD_PER_LONG	(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))
 
+#ifdef CONFIG_BLK_DEV_ZONED
+enum blk_zone_type {
+	BLK_ZONE_TYPE_UNKNOWN,
+	BLK_ZONE_TYPE_CONVENTIONAL,
+	BLK_ZONE_TYPE_SEQWRITE_REQ,
+	BLK_ZONE_TYPE_SEQWRITE_PREF,
+	BLK_ZONE_TYPE_RESERVED,
+};
+
+enum blk_zone_state {
+	BLK_ZONE_UNKNOWN,
+	BLK_ZONE_NO_WP,
+	BLK_ZONE_OPEN,
+	BLK_ZONE_READONLY,
+	BLK_ZONE_OFFLINE,
+	BLK_ZONE_BUSY,
+};
+
+struct blk_zone {
+	struct rb_node node;
+	spinlock_t lock;
+	uint64_t start;
+	uint64_t len;
+	uint64_t wp;
+	uint64_t shadow_wp;
+	enum blk_zone_type type;
+	enum blk_zone_state state;
+};
+
+#define blk_zone_is_smr(z) ((z)->type == BLK_ZONE_TYPE_SEQWRITE_REQ ||	\
+			    (z)->type == BLK_ZONE_TYPE_SEQWRITE_PREF)
+
+#define blk_zone_is_cmr(z) ((z)->type == BLK_ZONE_TYPE_CONVENTIONAL)
+#define blk_zone_is_full(z) ((z)->wp == (z)->start + (z)->len)
+#define blk_zone_is_empty(z) ((z)->wp == (z)->start)
+
+extern struct blk_zone *blk_lookup_zone(struct request_queue *, sector_t);
+extern struct blk_zone *blk_insert_zone(struct request_queue *,
+					struct blk_zone *);
+extern void blk_drop_zones(struct request_queue *);
+extern sector_t blk_zone_get_wp(struct blk_zone *);
+extern void blk_zone_copy(struct blk_zone *, struct blk_zone *);
+#else
+static inline void blk_drop_zones(struct request_queue *q) { };
+#endif
+
 struct queue_limits {
 	unsigned long		bounce_pfn;
 	unsigned long		seg_boundary_mask;
@@ -336,6 +383,7 @@ struct request_queue {
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 	lld_busy_fn		*lld_busy_fn;
+	chunk_limits_fn		*chunk_limits_fn;
 
 	struct blk_mq_ops	*mq_ops;
 
@@ -446,6 +494,12 @@ struct request_queue {
 
 	struct queue_limits	limits;
 
+#ifdef CONFIG_BLK_DEV_ZONED
+	struct rb_root		zones;
+	struct blk_zone		*zone_cache;
+	size_t			zone_len;
+#endif
+
 	/*
 	 * sg stuff
 	 */
@@ -711,6 +765,8 @@ static inline bool blk_write_same_mergeable(struct bio *a, struct bio *b)
 #define BLKPREP_OK		0	/* serve it */
 #define BLKPREP_KILL		1	/* fatal error, kill */
 #define BLKPREP_DEFER		2	/* leave on queue */
+#define BLKPREP_DONE		3	/* complete w/o error */
+#define BLKPREP_REQUEUE		4	/* put back to the end of the queue */
 
 extern unsigned long blk_max_low_pfn, blk_max_pfn;
 
@@ -800,6 +856,7 @@ extern struct request *blk_make_request(struct request_queue *, struct bio *,
 					gfp_t);
 extern void blk_rq_set_block_pc(struct request *);
 extern void blk_requeue_request(struct request_queue *, struct request *);
+extern void blk_defer_request(struct request_queue *, struct request *);
 extern void blk_add_request_payload(struct request *rq, struct page *page,
 		unsigned int len);
 extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
@@ -921,9 +978,12 @@ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
 static inline unsigned int blk_max_size_offset(struct request_queue *q,
 					       sector_t offset)
 {
-	if (!q->limits.chunk_sectors)
+	if (!q->limits.chunk_sectors && !q->chunk_limits_fn)
 		return q->limits.max_sectors;
 
+	if (q->chunk_limits_fn)
+		return q->chunk_limits_fn(q, offset);
+
 	return q->limits.chunk_sectors -
 			(offset & (q->limits.chunk_sectors - 1));
 }
@@ -935,7 +995,7 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
 	if (unlikely(rq->cmd_type == REQ_TYPE_BLOCK_PC))
 		return q->limits.max_hw_sectors;
 
-	if (!q->limits.chunk_sectors)
+	if (!q->limits.chunk_sectors && !q->chunk_limits_fn)
 		return blk_queue_get_max_sectors(q, rq->cmd_flags);
 
 	return min(blk_max_size_offset(q, blk_rq_pos(rq)),
@@ -1006,6 +1066,7 @@ extern void blk_queue_bounce_limit(struct request_queue *, u64);
 extern void blk_limits_max_hw_sectors(struct queue_limits *, unsigned int);
 extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
 extern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);
+extern void blk_queue_chunk_limits(struct request_queue *, chunk_limits_fn *);
 extern void blk_queue_max_segments(struct request_queue *, unsigned short);
 extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
 extern void blk_queue_max_discard_sectors(struct request_queue *q,
diff --git a/include/linux/elevator.h b/include/linux/elevator.h
index 45a9147..574ca35 100644
--- a/include/linux/elevator.h
+++ b/include/linux/elevator.h
@@ -127,7 +127,7 @@ extern void elv_merge_requests(struct request_queue *, struct request *,
 extern void elv_merged_request(struct request_queue *, struct request *, int);
 extern void elv_bio_merged(struct request_queue *q, struct request *,
 				struct bio *);
-extern void elv_requeue_request(struct request_queue *, struct request *);
+extern void elv_requeue_request(struct request_queue *, struct request *, bool);
 extern struct request *elv_former_request(struct request_queue *, struct request *);
 extern struct request *elv_latter_request(struct request_queue *, struct request *);
 extern int elv_register_queue(struct request_queue *q);
@@ -188,6 +188,7 @@ extern struct request *elv_rb_find(struct rb_root *, sector_t);
 #define ELEVATOR_INSERT_REQUEUE	4
 #define ELEVATOR_INSERT_FLUSH	5
 #define ELEVATOR_INSERT_SORT_MERGE	6
+#define ELEVATOR_INSERT_DEFER	7
 
 /*
  * return values from elevator_may_queue_fn

